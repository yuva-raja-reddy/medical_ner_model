{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "download_path = os.getcwd()\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"finalepoch/medical-ner\")\n",
    "\n",
    "destination_path = os.path.join(download_path, \"dataset\")\n",
    "shutil.move(dataset_path, destination_path)\n",
    "\n",
    "print(\"Dataset moved to:\", destination_path)\n",
    "dataset_path = \"dataset/Corona2.json\" \n",
    "data = pd.read_json(dataset_path)\n",
    "data.head()\n",
    "\n",
    "list(data['examples'][0].keys())\n",
    "\n",
    "data['examples'][0]['content']\n",
    "\n",
    "data['examples'][0]['annotations'][0]\n",
    "\n",
    "training_data = [{'text': example['content'],\n",
    "                  'entities': [(annotation['start'], annotation['end'], annotation['tag_name'].upper())\n",
    "                               for annotation in example['annotations']]}\n",
    "                 for example in data['examples']]\n",
    "\n",
    "training_data[0]['entities']\n",
    "\n",
    "training_data[0]['text'][563:571]\n",
    "\n",
    "nlp = spacy.blank(\"en\") \n",
    "doc_bin = DocBin()\n",
    "\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "for training_example in tqdm(training_data):\n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.set_ents(filtered_ents)\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"train.spacy\")\n",
    "\n",
    "! python -m spacy init config config.cfg --lang en --pipeline ner --optimize efficiency\n",
    "\n",
    "! python -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./train.spacy\n",
    "\n",
    "nlp_trained_model = spacy.load(\"model-best\")\n",
    "\n",
    "doc = nlp_trained_model('''\n",
    "The patient was prescribed Aspirin for their heart condition.\n",
    "The doctor recommended Ibuprofen to alleviate the patient's headache.\n",
    "The patient is suffering from diabetes, and they need to take Metformin regularly.\n",
    "After the surgery, the patient experienced some post-operative complications, including infection.\n",
    "The patient is currently on a regimen of Lisinopril to manage their high blood pressure.\n",
    "The antibiotic course for treating the bacterial infection should be completed as prescribed.\n",
    "The patient's insulin dosage needs to be adjusted to better control their blood sugar levels.\n",
    "The physician suspects that the patient may have pneumonia and has ordered a chest X-ray.\n",
    "The patient's cholesterol levels are high, and they have been advised to take Atorvastatin.\n",
    "The allergy to penicillin was noted in the patient's medical history.\n",
    "''')\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import docx  # python-docx for DOCX extraction\n",
    "import pytesseract  # OCR for images\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load the trained spaCy model\n",
    "nlp_trained_model = spacy.load(\"model-best\")  # Update path if needed\n",
    "\n",
    "# ============================================\n",
    "# Extract Text from PDFs, DOCX, Images, and Text\n",
    "# ============================================\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    \"\"\"Extract text from a DOCX file.\"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from an image using Tesseract OCR.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"Detect file type and extract text accordingly.\"\"\"\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif ext in [\"png\", \"jpg\", \"jpeg\"]:\n",
    "        return extract_text_from_image(file_path)\n",
    "    elif ext == \"txt\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    else:\n",
    "        return \"Unsupported file format.\"\n",
    "\n",
    "# ============================================\n",
    "# Named Entity Recognition (NER) Extraction\n",
    "# ============================================\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract named entities using the trained spaCy model.\"\"\"\n",
    "    doc = nlp_trained_model(text)\n",
    "    entities = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# ============================================\n",
    "# Gradio Interface to Upload Files or Text\n",
    "# ============================================\n",
    "\n",
    "def process_input(file=None, text=None):\n",
    "    \"\"\"Process input file or text and extract named entities.\"\"\"\n",
    "    \n",
    "    if file:\n",
    "        extracted_text = extract_text_from_file(file.name)\n",
    "    elif text:\n",
    "        extracted_text = text\n",
    "    else:\n",
    "        return \"No input provided\", []\n",
    "\n",
    "    entities = extract_entities(extracted_text)\n",
    "    return extracted_text, entities\n",
    "\n",
    "# Define Gradio Interface\n",
    "interface = gr.Interface(\n",
    "    fn=process_input,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload a File (PDF, DOCX, Image, or TXT)\", optional=True),\n",
    "        gr.Textbox(label=\"Or Paste Text Directly\", optional=True)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Extracted Text\"),\n",
    "        gr.JSON(label=\"Named Entities\")\n",
    "    ],\n",
    "    title=\"ðŸ“‘ NER Model: Extract Entities from Text, PDF, DOCX, and Images\",\n",
    "    description=\"Upload a document (TXT, PDF, DOCX, or Image) or enter text to extract named entities using the trained NER model.\",\n",
    ")\n",
    "\n",
    "# Run Gradio App\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/Users/yuvaraj/Desktop/projects/spacy model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Replace 'your_token_here' with your actual Hugging Face API token\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "repo_id = \"yuvarajareddy001/medical_ner_model\"  # Change this to your HF repo name\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"'/Users/yuvaraj/Desktop/projects/spacy model'\",  # Path to your trained spaCy model\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
